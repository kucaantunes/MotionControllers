MotionControllers

Source code:

https://drive.google.com/drive/folders/1gY2r6_ABqZbMkuwe0ia54R-PyIaWZRqm?usp=sharing


https://drive.google.com/drive/folders/1qzZbKDE_jOy_lQJlBgJAO_Epne4C3_6D?usp=sharing

Hand, body and eye movement controllers


![image](https://user-images.githubusercontent.com/26171557/187051082-ce8e3360-9f92-496e-ab35-e3ba493d9817.png)


With the objective to perform a more effective training for users of different ages and disabilities and to make the learning process easier, an online motion controller was implemented via web using several programming languages like JavaScript. 
Open CV is able to detect the players movement. Via python code it is possible to detect the position of the hands during web camera video streaming. This technology detects the hands movement while the user is interacting with the online platform, allowing to control virtual objects that interact with the system.
Another possibility of getting hand movement instead of a camera is by the use of specific sensors like leap motion that has a library that allows to use the Unity game engine to develop applications which can be used for different areas like training and healthcare.
With JavaScript it is possible to develop applications that interact with the movement of the hands in real time, facilitating the use for people that have visual limitations or that do not have a keyboard or a mouse.


![image](https://user-images.githubusercontent.com/26171557/187051101-3838e19f-d69b-4c03-ac79-50d558900bee.png)


![image](https://user-images.githubusercontent.com/26171557/187051103-a75f501c-d1ac-4ad3-98b9-98219589729a.png)


![image](https://user-images.githubusercontent.com/26171557/187051108-89c896c3-8516-49c2-a660-9ba463bcad86.png)


Learning activities can be more interactive and accessible for users of different ages and types of disabilities. Based on the context of training and to facilitate the performance of physiotherapeutic tasks a controller that detects, hands, arms, legs, head and feet was used to answer the third research question of this investigation.
Based on the creation of AI Agents, like the creation of a Computer-generated Imagery (CGI) girl, the sensor which in this case is the webcam requires to capture the all body in order to associate geometries and bones. A virtual girl replicates the user movements on a projected Liquid Crystal Display (LCD) that aims to help in physiotherapeutic terms by providing movements and exercises that can help some motor disabilities. The prototype was developed in C# using the Game Engine Unity providing a Mixed Reality (MR) experience. On a perspective of virtual storytelling, this project uses a process of providing interactivity to the user for story creation.

![image](https://user-images.githubusercontent.com/26171557/187051172-3ecf8ba0-f51e-4b7c-b426-c54dcbc2e1f0.png)


![image](https://user-images.githubusercontent.com/26171557/187051174-864f407a-19b5-4933-9fef-fc28ddd3a704.png)


Via the Unity IDE it is possible to configure the camera settings and the lightning and also to implement AI mechanisms like the behavior of the virtual elements among other possible functionalities. The programming language used to develop the prototype was C# and some libraries and classes were imported in order to implement all the features of a mixed reality first person shooter for mobile phones compatible with some of the operating systems of the android family.
   
   Preview of the prototype ARFPS.

Concerning the shooting system, a ray cast function was used taking in consideration direction, speed, position and distance and also spawner game object mechanisms in order for the bullets to appear after the verification of certain conditions like when the user presses the shooting button or when the AI agent or enemy is at a certain distance of the main player.
 
 Placing the camera in the Unity Game Engine

The animations follow a set of conditions and sequences in order to be activated, the picture below shows a schematic of the necessary order and trigger mechanisms associated to the animation transitions process during gameplay.

Making animation transitions in Unity

The animations follow an AI mechanism in order to provide more functionalities to MR systems as demonstrated in this prototype, it is possible to notice that the virtual elements can have more functionalities and interaction with the users. In the future will be possible that applications may have more uses from the real-world data captured by a sensor, the infrared (IR) cameras for example can provide night vision.
The process of treating information has been improving and the AI strategies nowadays are able to treat huger amounts of data making predictions with a high rate of accuracy.
The presented developed prototype illustrates a process of responding to the second research question by demonstrating a set of processes to achieve the proposed objective, for this demonstration was used the C# programming language and the Unity game engine among other technologies.


